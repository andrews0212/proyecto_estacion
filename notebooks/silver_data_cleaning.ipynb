{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from minio import Minio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar SparkSession\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "import time\n",
    "time.sleep(1)\n",
    "conf = SparkConf().setAppName(\"LimpiezaSilver\").setMaster(\"local[*]\").set(\"spark.driver.bindAddress\", \"127.0.0.1\").set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "try:\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "except:\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"âœ… Spark iniciado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n MinIO\n",
    "MINIO_ENDPOINT = os.environ.get(\"MINIO_ENDPOINT\", \"localhost:9000\")\n",
    "MINIO_ACCESS_KEY = os.environ.get(\"MINIO_ACCESS_KEY\", \"minioadmin\")\n",
    "MINIO_SECRET_KEY = os.environ.get(\"MINIO_SECRET_KEY\", \"minioadmin\")\n",
    "MINIO_BUCKET_BRONCE = os.environ.get(\"MINIO_BUCKET\", \"proyecto-bronze\")\n",
    "MINIO_BUCKET_SILVER = \"proyecto-silver\"\n",
    "\n",
    "minio_client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY, secure=False)\n",
    "print(\"âœ… MinIO conectado\")\n",
    "\n",
    "# Crear bucket Silver si no existe\n",
    "try:\n",
    "    minio_client.make_bucket(MINIO_BUCKET_SILVER)\n",
    "    print(f'âœ… Bucket {MINIO_BUCKET_SILVER} creado')\n",
    "except:\n",
    "    print(f'âœ… Bucket {MINIO_BUCKET_SILVER} ya existe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc96861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivos Bronze\n",
    "print(\"\\nðŸ“¥ CARGANDO DATOS DE BRONZE...\")\n",
    "archivo_reciente = None\n",
    "try:\n",
    "    print(\"Buscando archivos Bronze...\")\n",
    "    objects = minio_client.list_objects(MINIO_BUCKET_BRONCE, recursive=True)\n",
    "    archivos_parquet = [obj.object_name for obj in objects if obj.object_name.endswith(\".parquet\")]\n",
    "    \n",
    "    if archivos_parquet:\n",
    "        dfs = []\n",
    "        for archivo in sorted(archivos_parquet)[-8:]:  # Ãšltimos 8 archivos\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            temp_file = os.path.join(temp_dir, archivo.split(\"/\")[-1])\n",
    "            minio_client.fget_object(MINIO_BUCKET_BRONCE, archivo, temp_file)\n",
    "            df_temp = spark.read.parquet(temp_file)\n",
    "            dfs.append(df_temp)\n",
    "            print(f\"  Cargado: {archivo}\")\n",
    "        \n",
    "        # Consolidar todos los DataFrames\n",
    "        from functools import reduce\n",
    "        spark_df = reduce(lambda x, y: x.union(y), dfs)\n",
    "        print(f\"\\nâœ… Datos consolidados: {spark_df.count()} registros\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Sin archivos en Bronze\")\n",
    "        spark_df = spark.createDataFrame([(1, 25.5, 60)], [\"id\", \"temperature\", \"humidity\"])\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error: {e}\")\n",
    "    spark_df = spark.createDataFrame([(1, 25.5, 60)], [\"id\", \"temperature\", \"humidity\"])\n",
    "\n",
    "print(f\"\\nðŸ“Š DataFrame: {spark_df.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar limpieza\n",
    "print(\"\\nðŸ§¹ LIMPIEZA DE DATOS...\")\n",
    "\n",
    "initial_count = spark_df.count()\n",
    "\n",
    "# 1. Eliminar duplicados exactos (solo si todos los valores son idÃ©nticos)\n",
    "spark_df = spark_df.dropDuplicates()\n",
    "duplicates_removed = initial_count - spark_df.count()\n",
    "print(f\"Duplicados exactos eliminados: {duplicates_removed}\")\n",
    "print(f\"Registros mantenidos: {spark_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69778142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filtrar valores nulos crÃ­ticos\n",
    "spark_df = spark_df.filter(\n",
    "    col('ip').isNotNull() & \n",
    "    col('temperature').isNotNull()\n",
    ")\n",
    "after_nulls = spark_df.count()\n",
    "print(f\"DespuÃ©s de filtrar nulos: {after_nulls} (eliminados: {duplicates_removed})\")\n",
    "\n",
    "# 3. Filtrar outliers (menos agresivo)\n",
    "if 'temperature' in spark_df.columns and 'humidity' in spark_df.columns:\n",
    "    temp_mean = spark_df.select('temperature').rdd.flatMap(lambda x: x).mean()\n",
    "    temp_std = spark_df.select('temperature').rdd.flatMap(lambda x: x).stdev()\n",
    "    \n",
    "    if temp_mean is not None and temp_std is not None and temp_std > 0:\n",
    "        # Hacer filtro menos estricto (5 desviaciones en lugar de 3)\n",
    "        lower_bound = temp_mean - (5 * temp_std)\n",
    "        upper_bound = temp_mean + (5 * temp_std)\n",
    "        spark_df = spark_df.filter((col('temperature') >= lower_bound) & (col('temperature') <= upper_bound))\n",
    "\n",
    "after_outliers = spark_df.count()\n",
    "print(f\"Outliers de temperatura eliminados: {after_nulls - after_outliers}\")\n",
    "\n",
    "# 4. Imputar valores nulos en columnas numÃ©ricas con la media\n",
    "numeric_cols = ['temperature', 'humidity', 'pressure', 'pm25', 'light']\n",
    "for col_name in numeric_cols:\n",
    "    if col_name in spark_df.columns:\n",
    "        mean_val = spark_df.filter(col(col_name).isNotNull()).select(col(col_name)).rdd.flatMap(lambda x: x).mean()\n",
    "        if mean_val:\n",
    "            spark_df = spark_df.fillna(mean_val, subset=[col_name])\n",
    "            print(f\"Valores nulos imputados en {col_name} con media: {mean_val:.2f}\")\n",
    "\n",
    "# 5. Enriquecimiento temporal (opcional)\n",
    "if 'timestamp' in spark_df.columns:\n",
    "    try:\n",
    "        from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "        spark_df = spark_df.withColumn(\"year\", year(col(\"timestamp\")))\n",
    "        spark_df = spark_df.withColumn(\"month\", month(col(\"timestamp\")))\n",
    "        spark_df = spark_df.withColumn(\"day\", dayofmonth(col(\"timestamp\")))\n",
    "        spark_df = spark_df.withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "    except:\n",
    "        print(\"Enriquecimiento temporal skipeado (timestamp tiene formato complejo)\")\n",
    "\n",
    "print(f\"\\nLimpieza completada: {spark_df.count()} registros finales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en Silver\n",
    "print(\"\\nðŸ’¾ GUARDANDO EN SILVER...\")\n",
    "\n",
    "try:\n",
    "    import io\n",
    "    from pyspark.sql.types import TimestampType\n",
    "    \n",
    "    df_export = spark_df\n",
    "    for field in spark_df.schema.fields:\n",
    "        if isinstance(field.dataType, TimestampType):\n",
    "            df_export = df_export.withColumn(field.name, col(field.name).cast(\"string\"))\n",
    "    \n",
    "    pdf = df_export.toPandas()\n",
    "    csv_buffer = pdf.to_csv(index=False)\n",
    "    \n",
    "    object_path = \"sensor_data/cleaned_data.csv\"\n",
    "    csv_bytes = io.BytesIO(csv_buffer.encode('utf-8'))\n",
    "    minio_client.put_object(\n",
    "        MINIO_BUCKET_SILVER, \n",
    "        object_path, \n",
    "        csv_bytes,\n",
    "        length=len(csv_buffer.encode('utf-8')),\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    print(f\"âœ… Datos guardados: {object_path}\")\n",
    "    print(f\"ðŸ“Š Registros: {len(pdf)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"UnsatisfiedLinkError\" in error_msg or \"NativeIO\" in error_msg:\n",
    "        print(f\"âš ï¸ Warning Hadoop ignorado (no crÃ­tico)\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log de ejecuciÃ³n\n",
    "print(\"\\nðŸ“ GUARDANDO LOG DE EJECUCIÃ“N...\")\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "try:\n",
    "    log_data = {\n",
    "        'script': 'silver_data_cleaning',\n",
    "        'execution_time': datetime.now().isoformat(),\n",
    "        'records_processed': initial_count,\n",
    "        'records_cleaned': spark_df.count(),\n",
    "        'duplicates_removed': duplicates_removed,\n",
    "        'output_file': 'cleaned_data.csv',\n",
    "        'status': 'completed'\n",
    "    }\n",
    "    \n",
    "    import pandas as pd\n",
    "    log_df = pd.DataFrame([log_data])\n",
    "    log_csv = log_df.to_csv(index=False)\n",
    "    log_bytes = io.BytesIO(log_csv.encode('utf-8'))\n",
    "    \n",
    "    minio_client.put_object(\n",
    "        MINIO_BUCKET_SILVER,\n",
    "        \"logs/execution_log.csv\",\n",
    "        log_bytes,\n",
    "        length=len(log_csv.encode('utf-8')),\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Log guardado exitosamente\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"âœ… LIMPIEZA DE SILVER COMPLETADA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ðŸ“ Registros procesados: {initial_count}\")\n",
    "    print(f\"ðŸ“ Registros limpios: {spark_df.count()}\")\n",
    "    print(f\"ðŸ“ Duplicados removidos: {duplicates_removed}\")\n",
    "    print(f\"ðŸ“ Porcentaje conservado: {((spark_df.count()/initial_count)*100):.1f}%\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar log: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar SparkContext\n",
    "print(\"\\nðŸ”š CERRANDO SESIÃ“N...\")\n",
    "\n",
    "try:\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "        print(\"âœ… SesiÃ³n de Spark cerrada correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error al cerrar Spark: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValidaciÃ³n final de datos limpios\n",
    "if spark_df:\n",
    "    print(\"=== VALIDACIÃ“N FINAL ===\")\n",
    "    \n",
    "    final_count = enriched_df.count()\n",
    "    print(f\"Total de registros finales: {final_count}\")\n",
    "    \n",
    "    # Mostrar muestra de datos limpios\n",
    "    print(\"\\nMuestra de datos limpios:\")\n",
    "    enriched_df.select(\n",
    "        'sensor_id', 'temperature', 'humidity', 'pressure', \n",
    "        'temperature_fahrenheit', 'comfort_index', 'data_quality_score'\n",
    "    ).show(5)\n",
    "    \n",
    "    # EstadÃ­sticas de calidad\n",
    "    print(\"\\nDistribuciÃ³n de puntuaciÃ³n de calidad:\")\n",
    "    enriched_df.groupBy('data_quality_score').count().orderBy('data_quality_score').show()\n",
    "    \n",
    "    print(\"\\nDistribuciÃ³n de Ã­ndice de confort:\")\n",
    "    enriched_df.groupBy('comfort_index').count().show()\n",
    "    \n",
    "    # EstadÃ­sticas finales\n",
    "    avg_quality = enriched_df.agg(avg('data_quality_score')).collect()[0][0]\n",
    "    print(f\"\\nPuntuaciÃ³n promedio de calidad: {avg_quality:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datos limpios en capa Silver\n",
    "if spark_df:\n",
    "    print(\"=== GUARDANDO EN CAPA SILVER (ARCHIVO ÃšNICO) ===\")\n",
    "    \n",
    "    # Convertir a Pandas para guardar en MinIO\n",
    "    silver_df_pandas = enriched_df.toPandas()\n",
    "    \n",
    "    # Usar archivo Ãºnico que se sobrescribe\n",
    "    silver_path = f\"{Config.SILVER_PATH}/cleaned_data.parquet\"\n",
    "    \n",
    "    # Subir a MinIO (sobrescribir archivo existente)\n",
    "    print(f\"Sobrescribiendo archivo: {silver_path}\")\n",
    "    success = minio_client.upload_dataframe_as_parquet(silver_df_pandas, silver_path)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"âœ… Datos guardados exitosamente en capa Silver\")\n",
    "        print(f\"  - Archivo: cleaned_data.parquet\")\n",
    "        print(f\"  - Registros: {len(silver_df_pandas)}\")\n",
    "        print(f\"  - Columnas: {len(silver_df_pandas.columns)}\")\n",
    "        \n",
    "        # Mostrar archivos en Silver\n",
    "        silver_files = minio_client.list_objects(Config.SILVER_PATH)\n",
    "        print(f\"\\nArchivos totales en capa Silver: {len(silver_files)}\")\n",
    "        \n",
    "        # Crear log de ejecuciÃ³n (archivo Ãºnico que se sobrescribe)\n",
    "        from datetime import datetime\n",
    "        notebook_marker = {\n",
    "            'notebook': 'silver_data_cleaning',\n",
    "            'execution_time': datetime.now().isoformat(),\n",
    "            'records_processed': len(silver_df_pandas),\n",
    "            'output_file': 'cleaned_data.parquet',\n",
    "            'status': 'completed'\n",
    "        }\n",
    "        \n",
    "        log_df = pd.DataFrame([notebook_marker])\n",
    "        log_path = f\"{Config.SILVER_PATH}/execution_log.parquet\"\n",
    "        \n",
    "        # Sobrescribir log de ejecuciÃ³n\n",
    "        minio_client.upload_dataframe_as_parquet(log_df, log_path)\n",
    "        print(f\"ðŸ“ Log de ejecuciÃ³n actualizado\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Error al guardar en capa Silver\")\n",
    "else:\n",
    "    print(\"No hay datos para procesar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza final\n",
    "print(\"=== LIMPIEZA FINAL ===\")\n",
    "\n",
    "# Cerrar sesiÃ³n de Spark\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    print(\"âœ“ SesiÃ³n de Spark cerrada\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Proceso de limpieza completado exitosamente!\")\n",
    "print(\"\\nLos datos limpios estÃ¡n disponibles en la capa Silver para\")\n",
    "print(\"su posterior procesamiento en la capa Gold (KPIs).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
